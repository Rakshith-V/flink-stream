{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUqxk4MKIE2s"
   },
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq torch==2.0.1 --progress-bar off\n",
    "!pip install -qqq transformers==4.33.2 --progress-bar off\n",
    "!pip install -qqq langchain==0.0.299 --progress-bar off\n",
    "!pip install -qqq chromadb==0.4.10 --progress-bar off\n",
    "!pip install -qqq xformers==0.0.21 --progress-bar off\n",
    "!pip install -qqq sentence_transformers==2.2.2 --progress-bar off\n",
    "!pip install -qqq tokenizers==0.14.0 --progress-bar off\n",
    "!pip install -qqq optimum==1.13.1 --progress-bar off\n",
    "!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off\n",
    "!pip install -qqq unstructured==0.10.16 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi5O0hrXIHFP"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_community --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjRe-LRbIKM9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "MODEL_NAME = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.0001\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAL8XR4fIPjb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "result = llm(\n",
    "    \"consider the following table and suggest me 1 single product or row which has more chances of getting bought based on ant time stat: event_time event_type product_id category_id category_code brand price user_id user_session 2019-11-01 00:00:00 UTC view 1003461 2053013555631882655 electronics.smartphone xiaomi 489.07 520088904 4d3b30da-a5e4-49df-b1a8-ba5943f1dd33 2019-11-01 00:00:00 UTC view 5000088 2053013566100866035 appliances.sewing_machine janome 293.65 530496790 8e5f4f83-366c-4f70-860e-ca7417414283 2019-11-01 00:00:01 UTC view 17302664 2053013553853497655 creed 28.31 561587266 755422e7-9040-477b-9bd2-6a6e8fd97387 2019-11-01 00:00:01 UTC view 3601530 2053013563810775923 appliances.kitchen.washer lg 712.87 518085591 3bfb58cd-7892-48cc-8020-2f17e6de6e7f consider this table values and suggest me a product which has more chances of getting bought\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rsc89EpIUAk"
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
